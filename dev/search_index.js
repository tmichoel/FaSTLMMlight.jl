var documenterSearchIndex = {"docs":
[{"location":"introduction/","page":"Introduction","title":"Introduction","text":"CurrentModule = FaSTLMMlight","category":"page"},{"location":"introduction/#Introduction","page":"Introduction","title":"Introduction","text":"","category":"section"},{"location":"introduction/","page":"Introduction","title":"Introduction","text":"We consider the setup of Lippert et al. (2011), where:","category":"page"},{"location":"introduction/","page":"Introduction","title":"Introduction","text":"yinmathbbR^n is a response vector with values for n samples,\nXinmathbbR^ntimes d is a matrix with data of d covariates (fixed effects) in the same n samples,\nKinmathbbR^ntimes n is a positive semi-definite sample similarity matrix, scaled such that mathrmtr(K)=n,\nmu is an (unknown) overall mean parameter,\nbetainmathbbR^d is the (unknown) vector of fixed effect weights,\nsigma^2 is the (unknown) variance explained by K,\nsigma_e^2 is the (unknown) residual error variance,\ndelta = fracsigma_e^2sigma^2 is the variance ratio,","category":"page"},{"location":"introduction/","page":"Introduction","title":"Introduction","text":"and y is distributed as","category":"page"},{"location":"introduction/","page":"Introduction","title":"Introduction","text":"y sim Nbigl(mu + Xbeta sigma^2 K + sigma_e^2 Ibigr) = Nbigl( Xbeta sigma^2 (K + delta I)bigr)","category":"page"},{"location":"introduction/","page":"Introduction","title":"Introduction","text":"where N denotes a multivariate normal distribution.","category":"page"},{"location":"introduction/","page":"Introduction","title":"Introduction","text":"By adding a column of ones to X we can absorb the parameter mu in the vector of fixed effect weights beta. High-level functions in this package all accept a parameter mean=true which will call this operation:","category":"page"},{"location":"introduction/","page":"Introduction","title":"Introduction","text":"create_covariate_matrix","category":"page"},{"location":"introduction/#FaSTLMMlight.create_covariate_matrix","page":"Introduction","title":"FaSTLMMlight.create_covariate_matrix","text":"create_covariate_matrix(X; mean = true, n = 1)\n\nCreate the covariate matrix from the provided covariates with an intercept column if mean=true.\n\n\n\n\n\n","category":"function"},{"location":"introduction/","page":"Introduction","title":"Introduction","text":"Low-level functions all work with the model where mu is not explicitly modelled and the unknown parameters are (betasigma^2delta). If mean=true then in the output the overall estimated mean corresponds to the first element beta1.","category":"page"},{"location":"introduction/","page":"Introduction","title":"Introduction","text":"The parameters (betasigma^2delta) are estimated by maximum-likelihood (or restricted maximum-likelihood, see below), that is, by minimizing the negative log-likelihood function","category":"page"},{"location":"introduction/","page":"Introduction","title":"Introduction","text":"mathcalL = logdetbigl sigma^2 (K + delta I) bigr + frac1sigma^2 bigllangle y-Xbeta (K + delta I)^-1 (y-Xbeta)bigrrangle","category":"page"},{"location":"introduction/","page":"Introduction","title":"Introduction","text":"where langle uvrangle=u^Tv=sum_i=1^n u_n v_n is the usual inner product on mathbbR^n; note that for any matrix AinmathbbR^ntimes n and vectors uvinmathbbR^n, langle uAvrangle=mathrmtr(vu^TA). ","category":"page"},{"location":"introduction/","page":"Introduction","title":"Introduction","text":"Analytic expressions for the maximum-likelihood estimates hatbeta and hatsigma^2 as a function of delta are easy to obtain:","category":"page"},{"location":"introduction/","page":"Introduction","title":"Introduction","text":"beginaligned\nhatbeta = bigl X^T(K+delta I)^-1Xbigr^-1\n  X^T(K+delta I)^-1 y \n  hatsigma^2 = frac1n bigllangle y-Xhatbeta (K+delta I)^-1 (y-Xhatbeta)bigrrangle \nendaligned","category":"page"},{"location":"introduction/","page":"Introduction","title":"Introduction","text":"Plugging these expressions into the negative log-likelihood results in a (non-convex) function of the parameter delta, which upto an additive constant is given by: ","category":"page"},{"location":"introduction/","page":"Introduction","title":"Introduction","text":"mathcalL(delta) = logdet (K+delta I) + n log hatsigma^2","category":"page"},{"location":"introduction/","page":"Introduction","title":"Introduction","text":"In Lippert et al. (2011), the eigenvalue decomposition of K is used to increase the efficiency of evaluating mathcalL(delta), and thereby speed up the process of finding the maximum-likelihood estimate hatdelta. However, their method still expresses the evaluation of hatbeta(delta) as the solution of a linear system (if the number of covariates d1). This implies that the gradient of mathcalL(delta) cannot be computed using automatic differentiation, which means that only gradient-free optimization algorithms can be used (Lippert et al. (2011) used a basic grid-based search).","category":"page"},{"location":"introduction/","page":"Introduction","title":"Introduction","text":"Instead, FaST-LMM light first uses the singular value decomposition of the fixed effects matrix X to express the negative log-likelihood on the space orthogonal to the columns of X (in other words, uses restricted maximum likelihood). Then the spectral decomposition of K on the restricted space is used in the same manner as in the original FaST-LMM method, which results in a restricted negative log-likelihood function mathcalL_R(delta) whose gradient can be evaluated using automatic differentiation.","category":"page"},{"location":"FaST-LMM-fullrank/","page":"FaST-LMM full rank","title":"FaST-LMM full rank","text":"CurrentModule = FaSTLMMlight","category":"page"},{"location":"FaST-LMM-fullrank/#FaST-LMM-full-rank","page":"FaST-LMM full rank","title":"FaST-LMM full rank","text":"","category":"section"},{"location":"FaST-LMM-fullrank/","page":"FaST-LMM full rank","title":"FaST-LMM full rank","text":"We first consider the scenario where K is defined by a square semi-positive definite matrix and K_22 has been computed by project_orth_covar. Following Lippert et al. (2011), we call this the \"full rank\" scenario, although neither K nor K_22 actually has to have full rank.","category":"page"},{"location":"FaST-LMM-fullrank/","page":"FaST-LMM full rank","title":"FaST-LMM full rank","text":"The spectral decomposition of K_22 can be written as K_22=U Lambda U^T, with UinmathbbR^(n-d)times (n-d) unitary and LambdainmathbbR^(n-d)times (n-d) diagonal with non-negative diagonal elements lambda_i=Lambda_iigeq 0, which we assume are ordered, lambda_1geqlambda_2geq dots geq lambda_n-dgeq 0","category":"page"},{"location":"FaST-LMM-fullrank/","page":"FaST-LMM full rank","title":"FaST-LMM full rank","text":"The columns of U are the eigenvectors of K_22, and we denote them as u_i inmathbbR^n-d, with K_22 u_i=lambda_i u_i. The matrix U can be used to rotate the data y_2:","category":"page"},{"location":"FaST-LMM-fullrank/","page":"FaST-LMM full rank","title":"FaST-LMM full rank","text":"tildey = U^T y_2 in mathbbR^n-d","category":"page"},{"location":"FaST-LMM-fullrank/","page":"FaST-LMM full rank","title":"FaST-LMM full rank","text":"with components","category":"page"},{"location":"FaST-LMM-fullrank/","page":"FaST-LMM full rank","title":"FaST-LMM full rank","text":"tildey_i = langle u_iy_2ranglequad i=1dotsn-d","category":"page"},{"location":"FaST-LMM-fullrank/","page":"FaST-LMM full rank","title":"FaST-LMM full rank","text":"These results allow to express the terms in mathcalell_R ","category":"page"},{"location":"FaST-LMM-fullrank/","page":"FaST-LMM full rank","title":"FaST-LMM full rank","text":"beginaligned\n   frac1n-dlogdet(K_22+delta I) = frac1n-dsum_i=1^n-d log(lambda_i+delta)\n  hatsigma^2 = frac1n-dlangle y_2(K_22+delta I)^-1 y_2rangle = frac1n-dsum_i=1^n-d frac1lambda_i+delta langle y_2u_irangle^2 = frac1n-dsum_i=1^n-d fractildey_ilambda_i+delta\nendaligned","category":"page"},{"location":"FaST-LMM-fullrank/","page":"FaST-LMM full rank","title":"FaST-LMM full rank","text":"The values of hatsigma^2 and ell_R  as a function of the parameter delta and vectors lambda and tilde y are computed by the functions","category":"page"},{"location":"FaST-LMM-fullrank/","page":"FaST-LMM full rank","title":"FaST-LMM full rank","text":"sigma2_mle_fullrank\nminus_log_like_fullrank","category":"page"},{"location":"FaST-LMM-fullrank/#FaSTLMMlight.sigma2_mle_fullrank","page":"FaST-LMM full rank","title":"FaSTLMMlight.sigma2_mle_fullrank","text":"sigma2_mle_fullrank(δ, λ, yr)\n\nCompute the MLE of the variance parameter given the variance ratio δ, the eigenvalues of the kernel matrix, and the rotated response vector.\n\n\n\n\n\n","category":"function"},{"location":"FaST-LMM-fullrank/#FaSTLMMlight.minus_log_like_fullrank","page":"FaST-LMM full rank","title":"FaSTLMMlight.minus_log_like_fullrank","text":"minus_log_like_fullrank(δ, λ, yr)\n\nCompute the minus log-likelihood of the model, scaled by the number of samples and without constant factors, given the variance ratio δ, the eigenvalues of the kernel matrix, the rotated response vector and (optionally) the rotated covariates.\n\n\n\n\n\n","category":"function"},{"location":"FaST-LMM-fullrank/","page":"FaST-LMM full rank","title":"FaST-LMM full rank","text":"The function ","category":"page"},{"location":"FaST-LMM-fullrank/","page":"FaST-LMM full rank","title":"FaST-LMM full rank","text":"fastlmm_fullrank\ndelta_mle_fullrank\nbeta_mle_fullrank_lazy\nsoftplus","category":"page"},{"location":"FaST-LMM-fullrank/#FaSTLMMlight.fastlmm_fullrank","page":"FaST-LMM full rank","title":"FaSTLMMlight.fastlmm_fullrank","text":"fastlmm_fullrank(y,K; covariates = [], mean = true)\n\nFor a linear mixed model / Gaussian process,\n\ny sim Nbigl(Xbeta sigma^2(K + delta I) bigr)\n\nwhere y is the response vector, X is a matrix of covariates,  and K is  a full-rank kernel matrix, compute the restricted maximum-likelihood estimates (REMLs) of the variance parameter sigma^2 and the variance ratio delta using FaST-LMM with a full-rank kernel matrix. Compared to the original FaST-LMM algorithm, we first project out the (optional) covariates, incl. an (optional) constant off-set (mean=true), from the response vector and the kernel matrix. This avoids all matrix computations in the variance parameter estimation. Estimates for the fixed effects beta are not computed.\n\n\n\n\n\n","category":"function"},{"location":"FaST-LMM-fullrank/#FaSTLMMlight.delta_mle_fullrank","page":"FaST-LMM full rank","title":"FaSTLMMlight.delta_mle_fullrank","text":"delta_mle_fullrank(λ, yr)\n\nCompute the MLE of the variance ratio δ given the eigenvalues of the kernel matrix and the rotated response vector by solving the non-convex optimization problem formulated in the FaST-LMM paper.\n\n\n\n\n\n","category":"function"},{"location":"FaST-LMM-fullrank/#FaSTLMMlight.beta_mle_fullrank_lazy","page":"FaST-LMM full rank","title":"FaSTLMMlight.beta_mle_fullrank_lazy","text":"beta_mle_fullrank_lazy(y, K, X, σ², δ)\n\nLazy implementation of the MLE of the fixed effects weights given the response vector y, the kernel matrix K, the covariates X, the variance parameter σ² and the variance ratio δ. This function does not use spectral factorization, and should only be applied once, using the MLEs of the variance parameters.\n\n\n\n\n\n","category":"function"},{"location":"FaST-LMM-fullrank/#FaSTLMMlight.softplus","page":"FaST-LMM full rank","title":"FaSTLMMlight.softplus","text":"softplus(x)\n\nCompute the softplus function, i.e. log(1 + exp(x)), element-wise.\n\n\n\n\n\n","category":"function"},{"location":"listfunctions/","page":"List of functions","title":"List of functions","text":"CurrentModule = FaSTLMMlight","category":"page"},{"location":"listfunctions/#List-of-package-functions","page":"List of functions","title":"List of package functions","text":"","category":"section"},{"location":"listfunctions/","page":"List of functions","title":"List of functions","text":"","category":"page"},{"location":"","page":"Contents","title":"Contents","text":"CurrentModule = FaSTLMMlight","category":"page"},{"location":"#FaST-LMM-light-Documentation","page":"Contents","title":"FaST-LMM light Documentation","text":"","category":"section"},{"location":"","page":"Contents","title":"Contents","text":"","category":"page"},{"location":"svd-fixed-effects/","page":"SVD of the fixed effects","title":"SVD of the fixed effects","text":"CurrentModule = FaSTLMMlight","category":"page"},{"location":"svd-fixed-effects/#SVD-of-the-fixed-effects","page":"SVD of the fixed effects","title":"SVD of the fixed effects","text":"","category":"section"},{"location":"svd-fixed-effects/","page":"SVD of the fixed effects","title":"SVD of the fixed effects","text":"We assume that the number of fixed effects is less than the number of  samples, d  n, and that the matrix X of fixed effects has full rank, mathrmrank(X)=d. We can then write the \"thin\" singular value decomposition of X as X = V_1 Gamma W^T, where V_1inmathbbR^ntimes d with V_1^TV_1=I, Gamma inmathbbR^dtimes d diagonal with gamma_i=Gamma_ii0 for all i, and WinmathbbR^dtimes d with W^TW=WW^T=I. Furthermore there exists V_2inmathbbR^ntimes (n-d) with V_2^TV_2=I, V_1^TV_2=0 and V_1V_1^T+V_2V_2^T=I, i.e. the matrix V=(V_1 V_2) is unitary, and V_1V_1^T and V_2V_2^T are the orthogonal projection matrices on the range and (left) null space of X, respectively.","category":"page"},{"location":"svd-fixed-effects/","page":"SVD of the fixed effects","title":"SVD of the fixed effects","text":"Using the singular value decomposition of X, we can write any matrix AinmathbbR^ntimes n and vector yinmathbbR^n in block matrix/vector notation as","category":"page"},{"location":"svd-fixed-effects/","page":"SVD of the fixed effects","title":"SVD of the fixed effects","text":"beginaligned\n  A =\n       beginpmatrix\n         A_11  A_12\n         A_21  A_22\n       endpmatrixtextwherequad\n        A_ij = V_i^T A V_j\n  y =\n      beginpmatrix\n        y_1 y_2 \n      endpmatrix textwherequad\n  y_i = V_i^T y\nendaligned","category":"page"},{"location":"svd-fixed-effects/","page":"SVD of the fixed effects","title":"SVD of the fixed effects","text":"Using standard results for the inverse and determinant of a block matrix, we have","category":"page"},{"location":"svd-fixed-effects/","page":"SVD of the fixed effects","title":"SVD of the fixed effects","text":"  logdet(A) = logdet(A_11) + logdet(A_22-A_21A_11^-1A_12) = log det(A_11) - logdet(A^-1_22)","category":"page"},{"location":"svd-fixed-effects/","page":"SVD of the fixed effects","title":"SVD of the fixed effects","text":"Furthermore, we have","category":"page"},{"location":"svd-fixed-effects/","page":"SVD of the fixed effects","title":"SVD of the fixed effects","text":"  X (X^T A X)^-1 X^T = V_1Gamma W^T (WGamma V_1^T A V_1Gamma W^T)^-1 W Gamma V_1^T  = V_1 (V_1^T A V_1)^-1 V_1^T = V_1 (A_11)^-1 V_1^T\nendaligned","category":"page"},{"location":"svd-fixed-effects/","page":"SVD of the fixed effects","title":"SVD of the fixed effects","text":"Using this for the specific case where A=(K+delta I)^-1, together with the identity y=(V_1V_1^T+V_2V_2^T)y = V_1y_1 + V_2y_2, in the equation ","category":"page"},{"location":"svd-fixed-effects/","page":"SVD of the fixed effects","title":"SVD of the fixed effects","text":"  bigllangle y-Xhatbeta A (y-Xhatbeta)bigrrangle= langle yA yrangle - langle y A X (X^TAX)^-1 X^T A yrangle\n  = bigllangle y bigl(A - A X (X^TAX)^-1 X^T Abigr) y bigrrangle","category":"page"},{"location":"svd-fixed-effects/","page":"SVD of the fixed effects","title":"SVD of the fixed effects","text":"gives","category":"page"},{"location":"svd-fixed-effects/","page":"SVD of the fixed effects","title":"SVD of the fixed effects","text":"beginaligned\n  bigllangle y-Xhatbeta A (y-Xhatbeta)bigrrangle\n  qquad=\n                    beginpmatrix\n                      y_1^T   y_2^T\n                    endpmatrix\n                    beginpmatrix\n                      A_11  A_12\n                      A_21  A_22\n                    endpmatrix\n                    beginpmatrix\n                      y_1 y_2\n                    endpmatrix\n  -beginpmatrix\n    y_1^T   y_2^T\n  endpmatrix\n   beginpmatrix\n     A_11 (A_11)^-1 A_11  A_11 (A_11)^-1 A_12\n     A_21A_11 (A_11)^-1  A_21 (A_11)^-1A_12\n   endpmatrix\n   beginpmatrix\n     y_1 y_2\n   endpmatrix \n  qquad= beginpmatrix\n    y_1^T   y_2^T\n  endpmatrix\n            beginpmatrix\n              0  0\n              0  A_22 - A_21 (A_11)^-1A_12\n            endpmatrix\n                        beginpmatrix\n                          y_1 y_2\n                        endpmatrix\n  qquad= langle y_2(A^-1)_22^-1 y_2rangle\nendaligned","category":"page"},{"location":"svd-fixed-effects/","page":"SVD of the fixed effects","title":"SVD of the fixed effects","text":"Because using the maximum-likelihood estimates hatbeta has the effect of projecting out the fixed effects from the response y in the residual variance, it is common to also remove the contribution of the fixed effect space from the determinant term in the log-likelihood, i.e. replace","category":"page"},{"location":"svd-fixed-effects/","page":"SVD of the fixed effects","title":"SVD of the fixed effects","text":"beginaligned\n  logdet(K+delta I) = -logdetbigl(K+delta I)^-1bigr = -logdet(A) \n  to logdet(A^-1_22) = logdetbiglK_22+delta Ibigr\nendaligned","category":"page"},{"location":"svd-fixed-effects/","page":"SVD of the fixed effects","title":"SVD of the fixed effects","text":"and consider the residual or restricted negative log-likelihood function","category":"page"},{"location":"svd-fixed-effects/","page":"SVD of the fixed effects","title":"SVD of the fixed effects","text":"  mathcalL_R(sigma^2delta) = logdetbiglsigma^2(K_22+delta I)bigr + frac1sigma^2 langle y_2(K_22+delta I)^-1 y_2rangle","category":"page"},{"location":"svd-fixed-effects/","page":"SVD of the fixed effects","title":"SVD of the fixed effects","text":"This results in the restricted  maximum-likelihood estimate","category":"page"},{"location":"svd-fixed-effects/","page":"SVD of the fixed effects","title":"SVD of the fixed effects","text":"  hatsigma^2 = frac1n-d langle y_2(K_22+delta I)^-1 y_2rangle","category":"page"},{"location":"svd-fixed-effects/","page":"SVD of the fixed effects","title":"SVD of the fixed effects","text":"which is n(n-d) times the unrestricted maximum-likelihood estimate. Plugging this in the restricted negative log-likelihood function gives, upto an additive constant","category":"page"},{"location":"svd-fixed-effects/","page":"SVD of the fixed effects","title":"SVD of the fixed effects","text":"  mathcalL_R(delta) = logdetbigl(K_22+delta Ibigr) + (n-d)log hatsigma^2","category":"page"},{"location":"svd-fixed-effects/","page":"SVD of the fixed effects","title":"SVD of the fixed effects","text":"Because the sample size n may be large, we scale this function by n-d to obtain","category":"page"},{"location":"svd-fixed-effects/","page":"SVD of the fixed effects","title":"SVD of the fixed effects","text":"ell_R(delta) = frac1n-d mathcalL_R(delta) = frac1n-dlogdetbigl(K_22+delta Ibigr) + log hatsigma^2","category":"page"},{"location":"svd-fixed-effects/","page":"SVD of the fixed effects","title":"SVD of the fixed effects","text":"which needs to be minimized to obtain the restricted maximum-likelihood estimate hatdelta.","category":"page"},{"location":"svd-fixed-effects/","page":"SVD of the fixed effects","title":"SVD of the fixed effects","text":"If K is given as a square matrix, the projection of y and K onto the space orthogonal to the columns of X is done by the function:","category":"page"},{"location":"svd-fixed-effects/","page":"SVD of the fixed effects","title":"SVD of the fixed effects","text":"project_orth_covar","category":"page"},{"location":"svd-fixed-effects/#FaSTLMMlight.project_orth_covar","page":"SVD of the fixed effects","title":"FaSTLMMlight.project_orth_covar","text":"project_orth_covar(y, K, X)\n\nProject out the covariates from the response vector and the kernel matrix.\n\n\n\n\n\n","category":"function"}]
}

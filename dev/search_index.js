var documenterSearchIndex = {"docs":
[{"location":"introduction/","page":"Introduction","title":"Introduction","text":"CurrentModule = FaSTLMMlight","category":"page"},{"location":"introduction/#Introduction","page":"Introduction","title":"Introduction","text":"","category":"section"},{"location":"introduction/","page":"Introduction","title":"Introduction","text":"We consider the setup of Lippert et al. (2011), where:","category":"page"},{"location":"introduction/","page":"Introduction","title":"Introduction","text":"yinmathbbR^n is a response vector with values for n samples,\nXinmathbbR^ntimes d is a matrix with data of d covariates (fixed effects) in the same n samples,\nKinmathbbR^ntimes n is a positive semi-definite sample similarity matrix, scaled such that mathrmtr(K)=n,\nbetainmathbbR^d is the (unknown) vector of fixed effect weights,\nsigma^2 is the (unknown) variance explained by K,\nsigma_e^2 is the (unknown) residual error variance,\ndelta = fracsigma_e^2sigma^2 is the variance ratio,","category":"page"},{"location":"introduction/","page":"Introduction","title":"Introduction","text":"and y is distributed as","category":"page"},{"location":"introduction/","page":"Introduction","title":"Introduction","text":"y sim Nbigl( Xbeta sigma^2 K + sigma_e^2 Ibigr) = Nbigl( Xbeta sigma^2 (K + delta I)bigr)","category":"page"},{"location":"introduction/","page":"Introduction","title":"Introduction","text":"where N denotes a multivariate normal distribution.","category":"page"},{"location":"introduction/","page":"Introduction","title":"Introduction","text":"The unknown parameters (betasigma^2delta) are estimated by maximum-likelihood (or restricted maximum-likelihood, see below), that is, by minimizing the negative log-likelihood function","category":"page"},{"location":"introduction/","page":"Introduction","title":"Introduction","text":"mathcalL = logdetbigl sigma^2 (K + delta I) bigr + frac1sigma^2 bigllangle y-Xbeta (K + delta I)^-1 (y-Xbeta)bigrrangle","category":"page"},{"location":"introduction/","page":"Introduction","title":"Introduction","text":"where langle uvrangle=u^Tv=sum_i=1^n u_n v_n is the usual inner product on mathbbR^n; note that for any matrix AinmathbbR^ntimes n and vectors uvinmathbbR^n, langle uAvrangle=mathrmtr(vu^TA). ","category":"page"},{"location":"introduction/","page":"Introduction","title":"Introduction","text":"Analytic expressions for the maximum-likelihood estimates hatbeta and hatsigma^2 as a function of delta are easy to obtain:","category":"page"},{"location":"introduction/","page":"Introduction","title":"Introduction","text":"beginaligned\nhatbeta = bigl X^T(K+delta I)^-1Xbigr^-1\n  X^T(K+delta I)^-1 y \n  hatsigma^2 = frac1n bigllangle y-Xhatbeta (K+delta I)^-1 (y-Xhatbeta)bigrrangle \nendaligned","category":"page"},{"location":"introduction/","page":"Introduction","title":"Introduction","text":"Plugging these expressions into the negative log-likelihood results in a (non-convex) function of the parameter delta, which upto an additive constant is given by: ","category":"page"},{"location":"introduction/","page":"Introduction","title":"Introduction","text":"mathcalL(delta) = logdet (K+delta I) + n log hatsigma^2","category":"page"},{"location":"introduction/","page":"Introduction","title":"Introduction","text":"In Lippert et al. (2011), the eigenvalue decomposition of K is used to increase the efficiency of evaluating mathcalL(delta), and thereby speed up the process of finding the maximum-likelihood estimate hatdelta. However, their method still expresses the evaluation of hatbeta(delta) as the solution of a linear system (if the number of covariates d1). This implies that the gradient of mathcalL(delta) cannot be computed using automatic differentiation, which means that only gradient-free optimization algorithms can be used (Lippert et al. (2011) used a basic grid-based search).","category":"page"},{"location":"introduction/","page":"Introduction","title":"Introduction","text":"Instead, FaST-LMM light first uses the singular value decomposition of the fixed effects matrix X to express the negative log-likelihood on the space orthogonal to the columns of X (in other words, uses restricted maximum likelihood). Then the spectral decomposition of K on the restricted space is used in the same manner as in the original FaST-LMM method, which results in a restricted negative log-likelihood function mathcalL_R(delta) whose gradient can be evaluated using automatic differentiation.","category":"page"},{"location":"FaST-LMM-fullrank/","page":"FaST-LMM full rank","title":"FaST-LMM full rank","text":"CurrentModule = FaSTLMMlight","category":"page"},{"location":"FaST-LMM-fullrank/#FaST-LMM-full-rank","page":"FaST-LMM full rank","title":"FaST-LMM full rank","text":"","category":"section"},{"location":"FaST-LMM-fullrank/","page":"FaST-LMM full rank","title":"FaST-LMM full rank","text":"The spectral decomposition of K_22 can be written as K_22=U Lambda U^T, with UinmathbbR^(n-d)times (n-d) unitary and LambdainmathbbR^(n-d)times (n-d) diagonal with non-negative diagonal elements lambda_i=Lambda_iigeq 0. In general, K_22 need not be full rank, we assume that mathrmrank(K_22)=rleq n-d, and that the eigenvalues are ordered, lambda_1geqlambda_2geq dots geq lambda_r  lambda_r+1 = dots = lambda_n-d = 0. We can hence also write K_22 = U_1Lambda_1U_1^T, where U_1inmathbbR^(n-d)times r with U_1^TU_1=I, Lambda_1inmathbbR^rtimes r diagonal with diagonal elements lambda_1dotslambda_r, and U = (U_1 U_2) with U_2^TU_2=I, U_1^TU_2=0 and U_1U_1^T+U_2U_2^T=I. The columns of U are the eigenvectors of K_22, and we denote them as u_i inmathbbR^n-d, with K_22 u_i=lambda_i u_i. These results allow to express the terms in mathcalL_R as","category":"page"},{"location":"FaST-LMM-fullrank/","page":"FaST-LMM full rank","title":"FaST-LMM full rank","text":"beginaligned\n   logdet(K_22+delta I) = sum_i=1^r log(lambda_i+delta) + (n-d-r)log(delta)\n  langle y_2(K_22+delta I)^-1 y_2rangle = sum_i=1^r frac1lambda_i+delta langle y_2u_irangle^2 + frac1delta langle y_2U_2U_2^Ty_2rangle\n  = sum_i=1^r frac1lambda_i+delta langle y_2u_irangle^2 + frac1delta U_2^Ty_2^2\nendaligned","category":"page"},{"location":"FaST-LMM-fullrank/","page":"FaST-LMM full rank","title":"FaST-LMM full rank","text":"fastlmm_fullrank\ndelta_mle_fullrank\nminus_log_like_fullrank\nsigma2_mle_fullrank\nbeta_mle_fullrank_lazy\ncreate_covariate_matrix\nproject_orth_covar\nsoftplus","category":"page"},{"location":"FaST-LMM-fullrank/#FaSTLMMlight.fastlmm_fullrank","page":"FaST-LMM full rank","title":"FaSTLMMlight.fastlmm_fullrank","text":"fastlmm_fullrank(y,K; covariates = [], mean = true)\n\nFor a linear mixed model / Gaussian process,\n\ny sim Nbigl(Xbeta sigma^2(K + delta I) bigr)\n\nwhere y is the response vector, X is a matrix of covariates,  and K is  a full-rank kernel matrix, compute the restricted maximum-likelihood estimates (REMLs) of the variance parameter sigma^2 and the variance ratio delta using FaST-LMM with a full-rank kernel matrix. Compared to the original FaST-LMM algorithm, we first project out the (optional) covariates, incl. an (optional) constant off-set (mean=true), from the response vector and the kernel matrix. This avoids all matrix computations in the variance parameter estimation. Estimates for the fixed effects beta are not computed.\n\n\n\n\n\n","category":"function"},{"location":"FaST-LMM-fullrank/#FaSTLMMlight.delta_mle_fullrank","page":"FaST-LMM full rank","title":"FaSTLMMlight.delta_mle_fullrank","text":"delta_mle_fullrank(λ, yr)\n\nCompute the MLE of the variance ratio δ given the eigenvalues of the kernel matrix and the rotated response vector by solving the non-convex optimization problem formulated in the FaST-LMM paper.\n\n\n\n\n\n","category":"function"},{"location":"FaST-LMM-fullrank/#FaSTLMMlight.minus_log_like_fullrank","page":"FaST-LMM full rank","title":"FaSTLMMlight.minus_log_like_fullrank","text":"minus_log_like_fullrank(δ, λ, yr)\n\nCompute the minus log-likelihood of the model, scaled by the number of samples and without constant factors, given the variance ratio δ, the eigenvalues of the kernel matrix, the rotated response vector and (optionally) the rotated covariates.\n\n\n\n\n\n","category":"function"},{"location":"FaST-LMM-fullrank/#FaSTLMMlight.sigma2_mle_fullrank","page":"FaST-LMM full rank","title":"FaSTLMMlight.sigma2_mle_fullrank","text":"sigma2_mle_fullrank(δ, λ, yr)\n\nCompute the MLE of the variance parameter given the variance ratio δ, the eigenvalues of the kernel matrix, and the rotated response vector.\n\n\n\n\n\n","category":"function"},{"location":"FaST-LMM-fullrank/#FaSTLMMlight.beta_mle_fullrank_lazy","page":"FaST-LMM full rank","title":"FaSTLMMlight.beta_mle_fullrank_lazy","text":"beta_mle_fullrank_lazy(y, K, X, σ², δ)\n\nLazy implementation of the MLE of the fixed effects weights given the response vector y, the kernel matrix K, the covariates X, the variance parameter σ² and the variance ratio δ. This function does not use spectral factorization, and should only be applied once, using the MLEs of the variance parameters.\n\n\n\n\n\n","category":"function"},{"location":"FaST-LMM-fullrank/#FaSTLMMlight.create_covariate_matrix","page":"FaST-LMM full rank","title":"FaSTLMMlight.create_covariate_matrix","text":"create_covariate_matrix(X; mean = true, n = 1)\n\nCreate the covariate matrix from the provided covariates with an intercept column if mean=true.\n\n\n\n\n\n","category":"function"},{"location":"FaST-LMM-fullrank/#FaSTLMMlight.project_orth_covar","page":"FaST-LMM full rank","title":"FaSTLMMlight.project_orth_covar","text":"project_orth_covar(y, K, X)\n\nProject out the covariates from the response vector and the kernel matrix.\n\n\n\n\n\n","category":"function"},{"location":"FaST-LMM-fullrank/#FaSTLMMlight.softplus","page":"FaST-LMM full rank","title":"FaSTLMMlight.softplus","text":"softplus(x)\n\nCompute the softplus function, i.e. log(1 + exp(x)), element-wise.\n\n\n\n\n\n","category":"function"},{"location":"listfunctions/","page":"List of functions","title":"List of functions","text":"CurrentModule = FaSTLMMlight","category":"page"},{"location":"listfunctions/#List-of-package-functions","page":"List of functions","title":"List of package functions","text":"","category":"section"},{"location":"listfunctions/","page":"List of functions","title":"List of functions","text":"","category":"page"},{"location":"","page":"Contents","title":"Contents","text":"CurrentModule = FaSTLMMlight","category":"page"},{"location":"#FaST-LMM-light-Documentation","page":"Contents","title":"FaST-LMM light Documentation","text":"","category":"section"},{"location":"","page":"Contents","title":"Contents","text":"","category":"page"},{"location":"svd-fixed-effects/","page":"SVD of the fixed effects","title":"SVD of the fixed effects","text":"CurrentModule = FaSTLMMlight","category":"page"},{"location":"svd-fixed-effects/#SVD-of-the-fixed-effects","page":"SVD of the fixed effects","title":"SVD of the fixed effects","text":"","category":"section"},{"location":"svd-fixed-effects/","page":"SVD of the fixed effects","title":"SVD of the fixed effects","text":"We assume that the number of fixed effects is less than the number of  samples, d  n, and that the matrix X of fixed effects has full rank, mathrmrank(X)=d. We can then write the \"thin\" singular value decomposition of X as X = V_1 Gamma W^T, where V_1inmathbbR^ntimes d with V_1^TV_1=I, Gamma inmathbbR^dtimes d diagonal with gamma_i=Gamma_ii0 for all i, and WinmathbbR^dtimes d with W^TW=WW^T=I. Furthermore there exists V_2inmathbbR^ntimes (n-d) with V_2^TV_2=I, V_1^TV_2=0 and V_1V_1^T+V_2V_2^T=I, i.e. the matrix V=(V_1 V_2) is unitary, and V_1V_1^T and V_2V_2^T are the orthogonal projection matrices on the range and (left) null space of X, respectively.","category":"page"},{"location":"svd-fixed-effects/","page":"SVD of the fixed effects","title":"SVD of the fixed effects","text":"Using the singular value decomposition of X, we can write any matrix AinmathbbR^ntimes n and vector yinmathbbR^n in block matrix/vector notation as","category":"page"},{"location":"svd-fixed-effects/","page":"SVD of the fixed effects","title":"SVD of the fixed effects","text":"beginaligned\n  A =\n       beginpmatrix\n         A_11  A_12\n         A_21  A_22\n       endpmatrixtextwherequad\n        A_ij = V_i^T A V_j\n  y =\n      beginpmatrix\n        y_1 y_2 \n      endpmatrix textwherequad\n  y_i = V_i^T y\nendaligned","category":"page"},{"location":"svd-fixed-effects/","page":"SVD of the fixed effects","title":"SVD of the fixed effects","text":"Using standard results for the inverse and determinant of a block matrix, we have","category":"page"},{"location":"svd-fixed-effects/","page":"SVD of the fixed effects","title":"SVD of the fixed effects","text":"  logdet(A) = logdet(A_11) + logdet(A_22-A_21A_11^-1A_12) = log det(A_11) - logdet(A^-1_22)","category":"page"},{"location":"svd-fixed-effects/","page":"SVD of the fixed effects","title":"SVD of the fixed effects","text":"Furthermore, we have","category":"page"},{"location":"svd-fixed-effects/","page":"SVD of the fixed effects","title":"SVD of the fixed effects","text":"  X (X^T A X)^-1 X^T = V_1Gamma W^T (WGamma V_1^T A V_1Gamma W^T)^-1 W Gamma V_1^T  = V_1 (V_1^T A V_1)^-1 V_1^T = V_1 (A_11)^-1 V_1^T\nendaligned","category":"page"},{"location":"svd-fixed-effects/","page":"SVD of the fixed effects","title":"SVD of the fixed effects","text":"Using this for the specific case where A=(K+delta I)^-1, together with the identity y=(V_1V_1^T+V_2V_2^T)y = V_1y_1 + V_2y_2, in the equation ","category":"page"},{"location":"svd-fixed-effects/","page":"SVD of the fixed effects","title":"SVD of the fixed effects","text":"  bigllangle y-Xhatbeta A (y-Xhatbeta)bigrrangle= langle yA yrangle - langle y A X (X^TAX)^-1 X^T A yrangle\n  = bigllangle y bigl(A - A X (X^TAX)^-1 X^T Abigr) y bigrrangle","category":"page"},{"location":"svd-fixed-effects/","page":"SVD of the fixed effects","title":"SVD of the fixed effects","text":"gives","category":"page"},{"location":"svd-fixed-effects/","page":"SVD of the fixed effects","title":"SVD of the fixed effects","text":"beginaligned\n  bigllangle y-Xhatbeta A (y-Xhatbeta)bigrrangle\n  qquad=\n                    beginpmatrix\n                      y_1^T   y_2^T\n                    endpmatrix\n                    beginpmatrix\n                      A_11  A_12\n                      A_21  A_22\n                    endpmatrix\n                    beginpmatrix\n                      y_1 y_2\n                    endpmatrix\n  -beginpmatrix\n    y_1^T   y_2^T\n  endpmatrix\n   beginpmatrix\n     A_11 (A_11)^-1 A_11  A_11 (A_11)^-1 A_12\n     A_21A_11 (A_11)^-1  A_21 (A_11)^-1A_12\n   endpmatrix\n   beginpmatrix\n     y_1 y_2\n   endpmatrix \n  qquad= beginpmatrix\n    y_1^T   y_2^T\n  endpmatrix\n            beginpmatrix\n              0  0\n              0  A_22 - A_21 (A_11)^-1A_12\n            endpmatrix\n                        beginpmatrix\n                          y_1 y_2\n                        endpmatrix\n  qquad= langle y_2(A^-1)_22^-1 y_2rangle\nendaligned","category":"page"},{"location":"svd-fixed-effects/","page":"SVD of the fixed effects","title":"SVD of the fixed effects","text":"Because using the maximum-likelihood estimates hatbeta has the effect of projecting out the fixed effects from the response y in the residual variance, it is common to also remove the contribution of the fixed effect space from the determinant term in the log-likelihood, i.e. replace","category":"page"},{"location":"svd-fixed-effects/","page":"SVD of the fixed effects","title":"SVD of the fixed effects","text":"beginaligned\n  logdet(K+delta I) = -logdetbigl(K+delta I)^-1bigr = -logdet(A) \n  to logdet(A^-1_22) = logdetbiglK_22+delta Ibigr\nendaligned","category":"page"},{"location":"svd-fixed-effects/","page":"SVD of the fixed effects","title":"SVD of the fixed effects","text":"and consider the residual or restricted negative log-likelihood function","category":"page"},{"location":"svd-fixed-effects/","page":"SVD of the fixed effects","title":"SVD of the fixed effects","text":"  mathcalL_R(sigma^2delta) = logdetbiglsigma^2(K_22+delta I)bigr + frac1sigma^2 langle y_2(K_22+delta I)^-1 y_2rangle","category":"page"},{"location":"svd-fixed-effects/","page":"SVD of the fixed effects","title":"SVD of the fixed effects","text":"This results in the restricted  maximum-likelihood estimate","category":"page"},{"location":"svd-fixed-effects/","page":"SVD of the fixed effects","title":"SVD of the fixed effects","text":"  hatsigma^2 = frac1n-d langle y_2(K_22+delta I)^-1 y_2rangle","category":"page"},{"location":"svd-fixed-effects/","page":"SVD of the fixed effects","title":"SVD of the fixed effects","text":"which is n(n-d) times the unrestricted maximum-likelihood estimate. Plugging this in the restricted negative log-likelihood function gives, upto an additive constant","category":"page"},{"location":"svd-fixed-effects/","page":"SVD of the fixed effects","title":"SVD of the fixed effects","text":"  mathcalL_R(delta) = logdetbigl(K_22+delta Ibigr) + (n-d)log bigllangle y_2(K_22+delta I)^-1 y_2bigrrangle","category":"page"},{"location":"svd-fixed-effects/","page":"SVD of the fixed effects","title":"SVD of the fixed effects","text":"which needs to be minimized to obtain the restricted maximum-likelihood estimate hatdelta.","category":"page"}]
}
